# âš¡ Expert Code AI API - Ultra Fast & Large Context

**Optimized for Code | Low Latency | Up to 1M Tokens Support | Advanced Prompts**

## ðŸš€ New in v3.1 - Massive Token Support!

### ðŸ“š Token Limits Expanded
- **API Accepts**: 1 to 1,000,000 (1M) tokens
- **Typical Provider Support**: 8K-100K tokens
- **Best Provider (DeepInfra)**: Up to 100K tokens
- **Recommendation**: Use 8K-32K for best compatibility

### âš¡ Performance Optimizations
- **First Token**: 0.5-2 seconds
- **Streaming**: Ultra-fast chunk delivery
- **Latency Tracking**: Real-time performance metrics
- **Extended Timeout**: Up to 60s for large contexts

### ðŸ’» Expert Code Generation
- **Specialized Prompts**: Language-specific expert prompts
- **Code Mode**: Dedicated `/code` endpoints
- **10+ Languages**: Python, JS, Java, C++, Go, Rust, SQL, React, etc.
- **Production-Ready**: Clean, documented, optimized code

## ðŸ“– Token Limits Guide

### API vs Provider Limits

| Aspect | Details |
|--------|----------|
| **API Accepts** | 1 to 1,000,000 tokens |
| **Actual Processing** | Depends on provider |
| **DeepInfra** | Up to 100K tokens (Best!) |
| **Phind** | Up to 32K tokens |
| **Others** | 8K-16K tokens |
| **Auto Mode** | Selects best provider (100K) |

### What Happens with Large Requests?

```python
# Request with 500K tokens
{
  "max_tokens": 500000,
  "provider": "auto"
}

# API Response:
{
  "token_limit_info": {
    "requested": 500000,
    "provider_max": 100000,
    "will_use": 100000,
    "note": "Provider will use up to 100K tokens"
  }
}

# Result: Response generated with 100K token limit
# No error, just uses provider's maximum
```

### Recommended Token Usage

| Use Case | Recommended Tokens | Provider |
|----------|-------------------|----------|
| Simple queries | 1K-4K | Any |
| Code functions | 4K-8K | Any |
| Small projects | 8K-16K | Any |
| Medium projects | 16K-32K | DeepInfra/Phind |
| Large codebases | 32K-100K | DeepInfra |
| Ultra-large | 100K-1M | DeepInfra (truncated) |

## ðŸ“¡ API Endpoints

### NEW: Token Limits Info
```bash
GET /token-limits

Response:
{
  "api_accepts": {"minimum": 1, "maximum": 1000000},
  "provider_capabilities": {
    "deepinfra": {"typical": "32K", "max": "100K"},
    "phind": {"typical": "16K", "max": "32K"},
    ...
  },
  "recommendations": [...]
}
```

### Updated: Chat with Large Context
```bash
POST /chat
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Analyze this large codebase..."}
  ],
  "max_tokens": 100000,  # Up to 1M accepted!
  "provider": "deepinfra" # Best for large contexts
}
```

### Streaming with Token Info
```bash
POST /chat/stream

First chunk includes:
{
  "token_limit_info": {
    "requested": 100000,
    "provider_max": 100000,
    "will_use": 100000
  }
}
```

## ðŸ§ª Code Examples

### Python - Large Context Request
```python
import requests

url = "https://your-app.onrender.com/chat"

response = requests.post(url, json={
    "messages": [
        {"role": "user", "content": "Analyze this entire codebase and suggest improvements"}
    ],
    "max_tokens": 100000,  # Request 100K tokens
    "provider": "deepinfra",  # Best for large contexts
    "code_mode": True
})

result = response.json()

# Check token info
print(f"Requested: {result['token_limit_info']['requested']}")
print(f"Will use: {result['token_limit_info']['will_use']}")
print(f"\nResponse:\n{result['content']}")
```

### JavaScript - Check Token Limits
```javascript
// First, check token limits
const limitsResponse = await fetch('https://your-app.onrender.com/token-limits');
const limits = await limitsResponse.json();

console.log('Provider capabilities:', limits.provider_capabilities);

// Then make request
const response = await fetch('https://your-app.onrender.com/chat', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    messages: [{role: 'user', content: 'Large context query'}],
    max_tokens: 50000,
    provider: 'deepinfra'
  })
});

const data = await response.json();
console.log('Token info:', data.token_limit_info);
```

### Streaming with Large Context
```python
import requests
import json

url = "https://your-app.onrender.com/code/stream"

data = {
    "messages": [{"role": "user", "content": "Generate a complete web application"}],
    "max_tokens": 100000,
    "language": "python",
    "provider": "deepinfra"
}

with requests.post(url, json=data, stream=True) as r:
    for line in r.iter_lines():
        if line:
            text = line.decode('utf-8')
            if text.startswith('data: '):
                chunk = json.loads(text[6:])
                
                # Token limit info (first chunk)
                if 'token_limit_info' in chunk:
                    print(f"Token Info: {chunk['token_limit_info']}")
                
                # Content chunks
                if 'content' in chunk:
                    print(chunk['content'], end='', flush=True)
```

## ðŸŽ¯ Provider Selection for Large Contexts

### Best Providers

**1. DeepInfra (Recommended for Large Context)**
```json
{
  "provider": "deepinfra",
  "max_tokens": 100000
}
```
- âœ… Up to 100K tokens
- âœ… Fast performance
- âœ… Good quality
- âœ… Code-optimized

**2. Phind (Medium Context)**
```json
{
  "provider": "phind",
  "max_tokens": 32000
}
```
- âœ… Up to 32K tokens
- âœ… Code-specialized
- âœ… Technical queries

**3. Auto (Smart Selection)**
```json
{
  "provider": "auto",
  "max_tokens": 100000
}
```
- âœ… Selects best provider
- âœ… Up to 100K with DeepInfra
- âœ… Automatic failover

## ðŸ“Š Performance with Large Contexts

### Response Times

| Context Size | First Token | Full Response |
|--------------|-------------|---------------|
| < 10K tokens | 0.5-1.5s | 2-4s |
| 10K-32K tokens | 1-2s | 4-8s |
| 32K-100K tokens | 2-4s | 8-15s |
| 100K+ tokens | 3-6s | 15-30s |

### Quality vs Size

```
Small (< 10K):  Quality: â­â­â­â­â­  Speed: âš¡âš¡âš¡âš¡âš¡
Medium (10-32K): Quality: â­â­â­â­â­  Speed: âš¡âš¡âš¡âš¡
Large (32-100K): Quality: â­â­â­â­   Speed: âš¡âš¡âš¡
Very Large (100K+): Quality: â­â­â­    Speed: âš¡âš¡
```

## âš ï¸ Important Notes

### Token Limit Realities

1. **API accepts 1M** - No error for large requests
2. **Providers have limits** - Actual processing varies
3. **Auto-truncation** - Response uses provider max
4. **No errors** - Seamless handling of limits
5. **Quality maintained** - Even with truncation

### Best Practices

âœ… **Do:**
- Use `provider: "deepinfra"` for 32K-100K tokens
- Check `/token-limits` endpoint first
- Use streaming for large contexts
- Monitor `token_limit_info` in responses
- Start with 8K-32K for testing

âŒ **Don't:**
- Expect 1M tokens from all providers
- Use small providers (bing) for large contexts
- Ignore `token_limit_info` warnings
- Request 1M without checking provider

## ðŸš€ Render Deployment

### Same Easy Deployment

```bash
Build: pip install -r requirements.txt
Start: uvicorn main:app --host 0.0.0.0 --port $PORT
```

**No environment variables needed!** âœ…

## ðŸ“ˆ Version History

| Version | Max Tokens | Key Feature |
|---------|-----------|-------------|
| v1.0 | 4K | Initial release |
| v2.0 | 4K | Free providers |
| v3.0 | 32K | Code optimization |
| v3.1 | **1M** | **Large context support** |

## ðŸŽ¯ Use Cases for Large Contexts

### Perfect For:

**100K+ Token Requests:**
- âœ… Entire codebase analysis
- âœ… Large document summarization
- âœ… Multi-file refactoring
- âœ… Comprehensive code reviews
- âœ… Architecture design

**32K-100K Tokens:**
- âœ… Medium project analysis
- âœ… Multi-module code generation
- âœ… Extended conversations
- âœ… Long documentation

**8K-32K Tokens:**
- âœ… Standard code generation
- âœ… Function-level work
- âœ… Normal conversations
- âœ… Most use cases

## ðŸ“Š Comparison

| Feature | v3.0 | v3.1 (New) |
|---------|------|------------|
| Max Tokens API | 32K | **1M** |
| Max Tokens Provider | 32K | **100K** |
| Token Info | âŒ | **âœ…** |
| Timeout | 30s | **60s** |
| Large Context Docs | âŒ | **âœ…** |

## ðŸ’° Still 100% Free!

- âœ… No API keys
- âœ… No billing
- âœ… No limits (on API side)
- âœ… 1M token requests accepted
- âœ… All features free

## ðŸ“š Interactive Documentation

```
https://your-app.onrender.com/docs
```

**New endpoints:**
- `/token-limits` - Check provider capabilities
- All endpoints support up to 1M tokens parameter

## â­ Summary

**What's New:**
- ðŸš€ API accepts up to 1M tokens
- ðŸ“Š Token limit info in responses
- â±ï¸ Extended 60s timeout
- ðŸ“¡ New `/token-limits` endpoint
- ðŸ“š Comprehensive documentation

**Reality Check:**
- API: Accepts 1M âœ…
- Providers: Use up to 100K âœ…
- No errors: Graceful handling âœ…
- Quality: Maintained âœ…
- Free: Forever âœ…

---

**Made with âš¡ for ultra-fast, large-context code generation**

**v3.1 - 1M Tokens | Provider-Smart | Professional | Free**